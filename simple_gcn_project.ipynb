{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "NUZYIB2tlIVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "from typing import List, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\", category=DeprecationWarning,\n",
        ")\n"
      ],
      "metadata": {
        "id": "UqVgdpa7lHkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Seed"
      ],
      "metadata": {
        "id": "sJNP-OBtScSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "  np.random.seed(seed)\n",
        "  torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "szna4OhfSbtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models\n",
        "\n",
        "GCN - Graph Convolutional Network uses two graph convolution layers and a dropout layer.\n",
        "\n",
        "SGC - Simple Graph Convolution Network using a simple PyTorch implementation of logistic regression.\n",
        "\n",
        "Assumption of model SGC model use:\n",
        "- Features have been preprocessed with k-step graph propagation"
      ],
      "metadata": {
        "id": "zbHxKOABkjK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGC(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super(SGC, self).__init__()\n",
        "    # setup as fully connected network\n",
        "    self.fc1 = nn.Linear(in_features,\n",
        "                         out_features)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # forward feed the data through the fc layer\n",
        "    x = self.fc1(x)\n",
        "    return x\n",
        "\n",
        "class GCL(nn.modules.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super(GCL, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.weight = nn.parameter.Parameter(\n",
        "        torch.FloatTensor(\n",
        "            in_features,\n",
        "            out_features\n",
        "            )\n",
        "        )\n",
        "    self.bias = nn.parameter.Parameter(\n",
        "        torch.FloatTensor(out_features))\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "    self.weight.data.uniform_(-stdv, stdv)\n",
        "    self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, x, adj):\n",
        "    # forward feed the data through the fc layer\n",
        "    support = torch.mm(x, self.weight)\n",
        "    output = torch.spmm(adj, support)\n",
        "    return output + self.bias\n",
        "\n",
        "class GCN(nn.Module):\n",
        "  def __init__(self, in_features, in_hidden,\n",
        "               out_features, dropout):\n",
        "    super(GCN, self).__init__()\n",
        "    self.gc1 = GCL(in_features, in_hidden)\n",
        "    self.gc2 = GCL(in_hidden, out_features)\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def forward(self, x, adj):\n",
        "    x = self.gc1(x, adj)\n",
        "    x = F.relu(x)\n",
        "    x = F.dropout(x, self.dropout,\n",
        "                  training=self.training)\n",
        "    x = self.gc2(x, adj)\n",
        "    return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "Ob1zkz23kf8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization\n",
        "\n",
        "Graphs should be normalized.\n",
        "\n",
        "Reference: https://github.com/Tiiiger/SGC/blob/master/normalization.py"
      ],
      "metadata": {
        "id": "RI0IrGhupfgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "  sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "  indices = torch.from_numpy(\n",
        "      np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "  values = torch.from_numpy(sparse_mx.data)\n",
        "  shape = torch.Size(sparse_mx.shape)\n",
        "  return torch.sparse_coo_tensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def aug_normalized_adjacency(adj):\n",
        "  adj = sp.coo_matrix(adj)\n",
        "  adj = adj + sp.eye(adj.shape[0])\n",
        "  row_sum = np.array(adj.sum(1))\n",
        "  row_sum = (row_sum==0)*1 + row_sum\n",
        "  d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
        "  d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0\n",
        "  d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "  return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def row_normalize(mx):\n",
        "  \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "  rowsum = np.array(mx.sum(1))\n",
        "  rowsum = (rowsum==0)*1 + rowsum\n",
        "  r_inv = np.power(rowsum, -1).flatten()\n",
        "  r_inv[np.isinf(r_inv)] = 0.0\n",
        "  r_mat_inv = sp.diags(r_inv)\n",
        "  mx = r_mat_inv.dot(mx)\n",
        "  return mx"
      ],
      "metadata": {
        "id": "3M47Z13rppE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n",
        "\n",
        "Reference: https://github.com/Tiiiger/SGC/blob/master/metrics.py"
      ],
      "metadata": {
        "id": "3BTm6i0vr6t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, labels):\n",
        "  preds = output.max(1)[1].type_as(labels)\n",
        "  correct = preds.eq(labels).double()\n",
        "  correct = correct.sum()\n",
        "  return correct / len(labels)"
      ],
      "metadata": {
        "id": "lxYWJod_r95o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Datasets\n",
        "\n",
        "Modified from the reference: https://github.com/Tiiiger/SGC/blob/master/utils.py"
      ],
      "metadata": {
        "id": "kb4S7c5FsP8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_index_file(filename):\n",
        "  index = []\n",
        "  for line in open(filename):\n",
        "    index.append(int(line.strip()))\n",
        "  return index\n",
        "\n",
        "def sgc_precompute(features, adj, degree):\n",
        "  for i in range(degree):\n",
        "      features = torch.mm(adj, features)\n",
        "  return features"
      ],
      "metadata": {
        "id": "IJ7CT6NKBMQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_citation_sgc(path, dataset_str=\"cora\", normalization=\"AugNormAdj\", cuda=True):\n",
        "  \"\"\"\n",
        "  sgc: Load Citation Networks Datasets\n",
        "  \"\"\"\n",
        "  import pickle as pkl\n",
        "  import networkx as nx\n",
        "  names = [\"x\", \"y\", \"tx\", \"ty\", \"allx\", \"ally\", \"graph\"]\n",
        "  objects = []\n",
        "  for i in range(len(names)):\n",
        "    with open(\"{}/ind.{}.{}\".format(path, dataset_str.lower(), names[i]), \"rb\") as f:\n",
        "      objects.append(pkl.load(f, encoding=\"latin1\"))\n",
        "\n",
        "  x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "  test_idx_reorder = parse_index_file(\"{}/ind.{}.test.index\".format(path, dataset_str))\n",
        "  test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "  features = sp.vstack((allx, tx)).tolil()\n",
        "  features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "  adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "  labels = np.vstack((ally, ty))\n",
        "  labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "  idx_test = test_idx_range.tolist() # range(500,1500)\n",
        "  idx_train = range(len(y)) # range(140)\n",
        "  idx_val = range(len(y), len(y) + 500) # range(200,500)\n",
        "\n",
        "  features = row_normalize(features)\n",
        "\n",
        "  # porting to pytorch\n",
        "  features = torch.FloatTensor(np.array(features.todense())).float()\n",
        "  labels = torch.LongTensor(np.where(labels)[1])\n",
        "  adj = aug_normalized_adjacency(adj)\n",
        "  adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "  idx_train = torch.LongTensor(idx_train)\n",
        "  idx_val = torch.LongTensor(idx_val)\n",
        "  idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "  return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "metadata": {
        "id": "5aElu8L8sYmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train & Test Functions"
      ],
      "metadata": {
        "id": "2y8ZvQrsuDSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sgc(model: nn.Module,\n",
        "              loss_fn: nn.modules.loss._Loss,\n",
        "              optimizer: torch.optim.Optimizer,\n",
        "              train_features,\n",
        "              train_labels,\n",
        "              epoch: int=0):\n",
        "  # set the mode for the model\n",
        "  model.train()\n",
        "  # init optimizer as gradients accumulate\n",
        "  optimizer.zero_grad()\n",
        "  # feed forward the data into our network\n",
        "  output = model(train_features)\n",
        "  # compute the loss between the output of\n",
        "  # network and actual label\n",
        "  loss = loss_fn(output, train_labels)\n",
        "  # backwards propagation\n",
        "  loss.backward()\n",
        "  # move a step on the gradient by optimizer\n",
        "  optimizer.step()\n",
        "  # output some visual info\n",
        "  return loss.item()\n",
        "\n",
        "def test_sgc(model: nn.Module,\n",
        "         loss_fn: nn.modules.loss._Loss,\n",
        "         test_features,\n",
        "         test_labels,\n",
        "         epoch: int=0):\n",
        "  # set the model mode to test\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    output = model(test_features)\n",
        "    acc_val = accuracy(output, test_labels)\n",
        "  return acc_val\n",
        "\n",
        "def train_gcn(model: nn.Module,\n",
        "              optimizer: torch.optim.Optimizer,\n",
        "              features,\n",
        "              labels,\n",
        "              adj,\n",
        "              idx_train,\n",
        "              idx_val,\n",
        "              epoch: int=0):\n",
        "  # set the mode for the model\n",
        "  model.train()\n",
        "  # init optimizer as gradients accumulate\n",
        "  optimizer.zero_grad()\n",
        "  # feed forward the data into our network\n",
        "  output = model(features, adj)\n",
        "  # compute the loss between the output of\n",
        "  # network and actual label\n",
        "  loss_train = F.nll_loss(output[idx_train],\n",
        "                          labels[idx_train])\n",
        "  acc_train = accuracy(output[idx_train],\n",
        "                       labels[idx_train])\n",
        "  # backwards propagation\n",
        "  loss_train.backward()\n",
        "  # move a step on the gradient by optimizer\n",
        "  optimizer.step()\n",
        "  # record data every batch\n",
        "  loss_val = F.nll_loss(output[idx_val],\n",
        "                        labels[idx_val])\n",
        "  acc_val = accuracy(output[idx_val],\n",
        "                     labels[idx_val])\n",
        "  return loss_val.item()\n",
        "\n",
        "def test_gcn(model: nn.Module,\n",
        "             features,\n",
        "             labels,\n",
        "             adj,\n",
        "             idx_test,\n",
        "             epoch: int=0):\n",
        "  # set the model mode to test\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    output = model(features, adj)\n",
        "    loss_test = F.nll_loss(output[idx_test],\n",
        "                           labels[idx_test])\n",
        "    acc_val = accuracy(output[idx_test],\n",
        "                       labels[idx_test])\n",
        "  return acc_val"
      ],
      "metadata": {
        "id": "P1WJ8HpnuJaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive mount\n",
        "The datasets are mounted using Google Drive."
      ],
      "metadata": {
        "id": "y4LP_5Olq6xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twU2Bu4G00RO",
        "outputId": "362874c1-fe63-4dbc-f0fa-8bd70790630e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration and Run\n",
        "This requires the `sgcn_data` folder with the `Cora` and `Pubmed` datasets. `sgcn_data` is just the `data` folder from SGC, reference: https://github.com/Tiiiger/SGC/tree/master/data."
      ],
      "metadata": {
        "id": "GX8yJtXgyaSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#configuration\n",
        "seed = 42 # random seed\n",
        "max_epoch = 100\n",
        "lr = 0.2 # initial learning rate\n",
        "weight_decay=5e-6 # L2 loss on parameters\n",
        "degree = 2 # degrees of approximation\n",
        "\n",
        "# set random and torch seed\n",
        "set_seed(seed)\n",
        "\n",
        "# load the citation data\n",
        "datapath = '/content/drive/MyDrive/colab_notebooks/ece570/project/sgcn_data/'\n",
        "dataset_list = [\"cora\", \"pubmed\"]\n",
        "for dataset in dataset_list:\n",
        "  print(f'** SGC training/testing on dataset {dataset} **')\n",
        "  # train and test SGC\n",
        "  time_list = []\n",
        "  test_acc_list = []\n",
        "  num_loops = 20\n",
        "  for loop in range(num_loops):\n",
        "    loss = 0\n",
        "    train_acc = 0\n",
        "    epoch = 1\n",
        "    adj, features, labels, idx_train, idx_val, idx_test = load_citation_sgc(datapath, dataset)\n",
        "    # move to gpu\n",
        "    features = features.to('cuda')\n",
        "    adj = adj.to('cuda')\n",
        "    labels = labels.to('cuda')\n",
        "    idx_train = idx_train.to('cuda')\n",
        "    idx_val = idx_val.to('cuda')\n",
        "    idx_test = idx_test.to('cuda')\n",
        "    # setup model\n",
        "    model = SGC(in_features=features.size(1), out_features=labels.max().item()+1)\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    start = time.time()\n",
        "    # SGC precomputation of features\n",
        "    features = sgc_precompute(features, adj, degree)\n",
        "    for epoch in range(1, max_epoch+1):\n",
        "      loss = loss + train_sgc(model, loss_fn, optimizer, features[idx_train], labels[idx_train], epoch)\n",
        "      train_acc = test_sgc(model, loss_fn, features[idx_val], labels[idx_val], epoch)\n",
        "    test_acc = test_sgc(model, loss_fn, features[idx_test], labels[idx_test], epoch)\n",
        "    end = time.time()\n",
        "    avg_loss = loss/100\n",
        "    train_test_time = end-start\n",
        "    time_list.append(train_test_time)\n",
        "    test_acc_list.append(test_acc.to('cpu'))\n",
        "    print(f'Loop {loop+1:2d}: Avg Loss: {avg_loss:.2f} Train Acc: {100*train_acc:.2f} '\n",
        "          f'Test Acc: {100*test_acc:.2f}, Train Time: {end-start:2f} s')\n",
        "  print(f'Avg Train/Test Time over {num_loops} loops: {np.mean(time_list):2f} s, Avg Test Acc: {100*np.mean(test_acc_list):.2f}%\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n-gRtYVydFr",
        "outputId": "da912279-8146-479f-d289-cdb7f9c07686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** SGC training/testing on dataset cora **\n",
            "Loop  1: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.30, Train Time: 0.143639 s\n",
            "Loop  2: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.70, Train Time: 0.143239 s\n",
            "Loop  3: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.70, Train Time: 0.139168 s\n",
            "Loop  4: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.70, Train Time: 0.145954 s\n",
            "Loop  5: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.60, Train Time: 0.140142 s\n",
            "Loop  6: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.60, Train Time: 0.133879 s\n",
            "Loop  7: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.50, Train Time: 0.142044 s\n",
            "Loop  8: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.30, Train Time: 0.145280 s\n",
            "Loop  9: Avg Loss: 0.37 Train Acc: 79.00 Test Acc: 80.60, Train Time: 0.140335 s\n",
            "Loop 10: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.40, Train Time: 0.143753 s\n",
            "Loop 11: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.30, Train Time: 0.147527 s\n",
            "Loop 12: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.70, Train Time: 0.142096 s\n",
            "Loop 13: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.30, Train Time: 0.156949 s\n",
            "Loop 14: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.70, Train Time: 0.169008 s\n",
            "Loop 15: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.60, Train Time: 0.184317 s\n",
            "Loop 16: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.70, Train Time: 0.170501 s\n",
            "Loop 17: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.50, Train Time: 0.158433 s\n",
            "Loop 18: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.40, Train Time: 0.165776 s\n",
            "Loop 19: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.50, Train Time: 0.172691 s\n",
            "Loop 20: Avg Loss: 0.37 Train Acc: 79.20 Test Acc: 80.70, Train Time: 0.157260 s\n",
            "Avg Train/Test Time over 20 loops: 0.152099 s, Avg Test Acc: 80.54%\n",
            "\n",
            "** SGC training/testing on dataset pubmed **\n",
            "Loop  1: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 77.90, Train Time: 0.141501 s\n",
            "Loop  2: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 78.00, Train Time: 0.136345 s\n",
            "Loop  3: Avg Loss: 0.23 Train Acc: 77.40 Test Acc: 77.90, Train Time: 0.133026 s\n",
            "Loop  4: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 77.70, Train Time: 0.138430 s\n",
            "Loop  5: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 78.00, Train Time: 0.139227 s\n",
            "Loop  6: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 78.00, Train Time: 0.136290 s\n",
            "Loop  7: Avg Loss: 0.23 Train Acc: 77.40 Test Acc: 77.90, Train Time: 0.135214 s\n",
            "Loop  8: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 77.90, Train Time: 0.131978 s\n",
            "Loop  9: Avg Loss: 0.23 Train Acc: 77.40 Test Acc: 77.90, Train Time: 0.136781 s\n",
            "Loop 10: Avg Loss: 0.23 Train Acc: 77.40 Test Acc: 77.70, Train Time: 0.186951 s\n",
            "Loop 11: Avg Loss: 0.23 Train Acc: 77.40 Test Acc: 77.80, Train Time: 0.161597 s\n",
            "Loop 12: Avg Loss: 0.23 Train Acc: 77.40 Test Acc: 77.80, Train Time: 0.240005 s\n",
            "Loop 13: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 78.00, Train Time: 0.222015 s\n",
            "Loop 14: Avg Loss: 0.23 Train Acc: 77.40 Test Acc: 77.90, Train Time: 0.136757 s\n",
            "Loop 15: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 78.00, Train Time: 0.139668 s\n",
            "Loop 16: Avg Loss: 0.23 Train Acc: 77.40 Test Acc: 77.90, Train Time: 0.135952 s\n",
            "Loop 17: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 77.70, Train Time: 0.140022 s\n",
            "Loop 18: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 77.90, Train Time: 0.138887 s\n",
            "Loop 19: Avg Loss: 0.23 Train Acc: 77.20 Test Acc: 77.70, Train Time: 0.135154 s\n",
            "Loop 20: Avg Loss: 0.23 Train Acc: 77.40 Test Acc: 77.90, Train Time: 0.136467 s\n",
            "Avg Train/Test Time over 20 loops: 0.150113 s, Avg Test Acc: 77.88%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configuration from pygcn\n",
        "seed = 42 # random seed\n",
        "max_epoch = 100\n",
        "lr = 0.01 # initial learning rate\n",
        "weight_decay=5e-4 # L2 loss on parameters\n",
        "hidden = 50 # number of hidden units\n",
        "dropout = 0.5 # dropout rate (1 - keep probability)\n",
        "\n",
        "# set random and torch seed\n",
        "set_seed(seed)\n",
        "\n",
        "# load the citation data\n",
        "datapath = '/content/drive/MyDrive/colab_notebooks/ece570/project/sgcn_data/'\n",
        "dataset_list = [\"cora\", \"pubmed\"]\n",
        "for dataset in dataset_list:\n",
        "  print(f'** GCN training/testing on dataset {dataset} **')\n",
        "  time_list = []\n",
        "  test_acc_list = []\n",
        "  num_loops = 20\n",
        "  for loop in range(num_loops):\n",
        "    loss = 0\n",
        "    train_acc = 0\n",
        "    epoch = 1\n",
        "    adj, features, labels, idx_train, idx_val, idx_test = load_citation_sgc(datapath, dataset)\n",
        "    # move to gpu\n",
        "    features = features.to('cuda')\n",
        "    adj = adj.to('cuda')\n",
        "    labels = labels.to('cuda')\n",
        "    idx_train = idx_train.to('cuda')\n",
        "    idx_val = idx_val.to('cuda')\n",
        "    idx_test = idx_test.to('cuda')\n",
        "    # setup model\n",
        "    model = GCN(in_features=features.shape[1], in_hidden=hidden, out_features=labels.max().item()+1, dropout=dropout)\n",
        "    model = model.to('cuda')\n",
        "    # setup optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # train and test GCN\n",
        "    start = time.time()\n",
        "    for epoch in range(1, max_epoch+1):\n",
        "      loss = loss + train_gcn(model, optimizer, features, labels, adj, idx_train, idx_val, epoch)\n",
        "      train_acc = test_gcn(model, features, labels, adj, idx_test, epoch)\n",
        "    test_acc = test_gcn(model, features, labels, adj, idx_test, epoch)\n",
        "    end = time.time()\n",
        "    avg_loss = loss/100\n",
        "    train_test_time = end-start\n",
        "    time_list.append(train_test_time)\n",
        "    test_acc_list.append(test_acc.to('cpu'))\n",
        "    print(f'Loop {loop+1:2d}: Avg Loss: {avg_loss:.2f} Train Accuracy: {100*train_acc:.2f} '\n",
        "          f'Test Accuracy: {100*test_acc:.2f}, Training Time: {end-start:2f} s')\n",
        "  print(f'Avg Train/Test Time over {num_loops} loops: {np.mean(time_list):2f} s, Avg Test Acc: {100*np.mean(test_acc_list):.2f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsd1xFVATJuc",
        "outputId": "5bfa4dfc-6a9c-480f-d35e-b0aa24a70caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** GCN training/testing on dataset cora **\n",
            "Loop  1: Avg Loss: 1.28 Train Accuracy: 81.30 Test Accuracy: 81.30, Training Time: 0.896144 s\n",
            "Loop  2: Avg Loss: 1.34 Train Accuracy: 82.10 Test Accuracy: 82.10, Training Time: 0.787615 s\n",
            "Loop  3: Avg Loss: 1.30 Train Accuracy: 81.60 Test Accuracy: 81.60, Training Time: 1.046986 s\n",
            "Loop  4: Avg Loss: 1.31 Train Accuracy: 81.20 Test Accuracy: 81.20, Training Time: 0.523376 s\n",
            "Loop  5: Avg Loss: 1.30 Train Accuracy: 81.60 Test Accuracy: 81.60, Training Time: 0.532498 s\n",
            "Loop  6: Avg Loss: 1.34 Train Accuracy: 81.10 Test Accuracy: 81.10, Training Time: 0.524721 s\n",
            "Loop  7: Avg Loss: 1.33 Train Accuracy: 81.70 Test Accuracy: 81.70, Training Time: 0.506311 s\n",
            "Loop  8: Avg Loss: 1.31 Train Accuracy: 80.50 Test Accuracy: 80.50, Training Time: 0.472215 s\n",
            "Loop  9: Avg Loss: 1.28 Train Accuracy: 80.80 Test Accuracy: 80.80, Training Time: 0.498422 s\n",
            "Loop 10: Avg Loss: 1.30 Train Accuracy: 81.20 Test Accuracy: 81.20, Training Time: 0.484762 s\n",
            "Loop 11: Avg Loss: 1.35 Train Accuracy: 81.10 Test Accuracy: 81.10, Training Time: 0.512920 s\n",
            "Loop 12: Avg Loss: 1.29 Train Accuracy: 81.30 Test Accuracy: 81.30, Training Time: 0.628712 s\n",
            "Loop 13: Avg Loss: 1.31 Train Accuracy: 81.20 Test Accuracy: 81.20, Training Time: 0.670595 s\n",
            "Loop 14: Avg Loss: 1.32 Train Accuracy: 80.80 Test Accuracy: 80.80, Training Time: 0.656443 s\n",
            "Loop 15: Avg Loss: 1.30 Train Accuracy: 81.80 Test Accuracy: 81.80, Training Time: 0.617902 s\n",
            "Loop 16: Avg Loss: 1.35 Train Accuracy: 81.10 Test Accuracy: 81.10, Training Time: 0.606499 s\n",
            "Loop 17: Avg Loss: 1.32 Train Accuracy: 81.80 Test Accuracy: 81.80, Training Time: 0.491261 s\n",
            "Loop 18: Avg Loss: 1.33 Train Accuracy: 81.10 Test Accuracy: 81.10, Training Time: 0.488840 s\n",
            "Loop 19: Avg Loss: 1.31 Train Accuracy: 81.20 Test Accuracy: 81.20, Training Time: 0.490848 s\n",
            "Loop 20: Avg Loss: 1.32 Train Accuracy: 81.50 Test Accuracy: 81.50, Training Time: 0.492454 s\n",
            "Avg Train/Test Time over 20 loops: 0.596476 s, Avg Test Acc: 81.30\n",
            "\n",
            "** GCN training/testing on dataset pubmed **\n",
            "Loop  1: Avg Loss: 0.72 Train Accuracy: 79.10 Test Accuracy: 79.10, Training Time: 0.751495 s\n",
            "Loop  2: Avg Loss: 0.71 Train Accuracy: 79.00 Test Accuracy: 79.00, Training Time: 0.686645 s\n",
            "Loop  3: Avg Loss: 0.75 Train Accuracy: 78.70 Test Accuracy: 78.70, Training Time: 0.712600 s\n",
            "Loop  4: Avg Loss: 0.71 Train Accuracy: 79.40 Test Accuracy: 79.40, Training Time: 0.758836 s\n",
            "Loop  5: Avg Loss: 0.73 Train Accuracy: 79.30 Test Accuracy: 79.30, Training Time: 0.803556 s\n",
            "Loop  6: Avg Loss: 0.70 Train Accuracy: 79.60 Test Accuracy: 79.60, Training Time: 0.707505 s\n",
            "Loop  7: Avg Loss: 0.72 Train Accuracy: 79.00 Test Accuracy: 79.00, Training Time: 0.713908 s\n",
            "Loop  8: Avg Loss: 0.73 Train Accuracy: 79.10 Test Accuracy: 79.10, Training Time: 0.802303 s\n",
            "Loop  9: Avg Loss: 0.73 Train Accuracy: 79.40 Test Accuracy: 79.40, Training Time: 0.707473 s\n",
            "Loop 10: Avg Loss: 0.73 Train Accuracy: 79.40 Test Accuracy: 79.40, Training Time: 0.702887 s\n",
            "Loop 11: Avg Loss: 0.72 Train Accuracy: 79.80 Test Accuracy: 79.80, Training Time: 0.704637 s\n",
            "Loop 12: Avg Loss: 0.72 Train Accuracy: 79.10 Test Accuracy: 79.10, Training Time: 0.817522 s\n",
            "Loop 13: Avg Loss: 0.71 Train Accuracy: 79.50 Test Accuracy: 79.50, Training Time: 0.799265 s\n",
            "Loop 14: Avg Loss: 0.72 Train Accuracy: 79.30 Test Accuracy: 79.30, Training Time: 0.701667 s\n",
            "Loop 15: Avg Loss: 0.72 Train Accuracy: 79.90 Test Accuracy: 79.90, Training Time: 0.714643 s\n",
            "Loop 16: Avg Loss: 0.74 Train Accuracy: 79.50 Test Accuracy: 79.50, Training Time: 0.690978 s\n",
            "Loop 17: Avg Loss: 0.73 Train Accuracy: 79.10 Test Accuracy: 79.10, Training Time: 0.690239 s\n",
            "Loop 18: Avg Loss: 0.72 Train Accuracy: 79.20 Test Accuracy: 79.20, Training Time: 0.703837 s\n",
            "Loop 19: Avg Loss: 0.73 Train Accuracy: 79.60 Test Accuracy: 79.60, Training Time: 0.739789 s\n",
            "Loop 20: Avg Loss: 0.72 Train Accuracy: 79.50 Test Accuracy: 79.50, Training Time: 0.808967 s\n",
            "Avg Train/Test Time over 20 loops: 0.735938 s, Avg Test Acc: 79.33\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extension Work\n",
        "\n",
        "Running SGC to compare against extended datasets from Geom-GCN and GCNII.\n",
        "\n",
        "Modified from reference: https://github.com/bingzhewei/geom-gcn/blob/master/utils_data.py"
      ],
      "metadata": {
        "id": "dJuhRNFgmjxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_extended_data(path, dataset_name, splits_file_name):\n",
        "  graph_adjacency_list_file_path = path + dataset_name + '/out1_graph_edges.txt'\n",
        "  graph_node_features_and_labels_file_path = path + dataset_name + '/out1_node_feature_label.txt'\n",
        "  splits_file_path = path + 'splits/' + dataset_name + splits_file_name\n",
        "\n",
        "  import networkx as nx\n",
        "  G = nx.DiGraph()\n",
        "  graph_node_features_dict = {}\n",
        "  graph_labels_dict = {}\n",
        "\n",
        "  with open(graph_node_features_and_labels_file_path, 'r', encoding='utf-8') as graph_node_features_and_labels_file:\n",
        "    graph_node_features_and_labels_file.readline()\n",
        "    for line in graph_node_features_and_labels_file:\n",
        "      line = line.rstrip().split('\\t')\n",
        "      assert (len(line) == 3)\n",
        "      assert (int(line[0]) not in graph_node_features_dict and int(line[0]) not in graph_labels_dict)\n",
        "      graph_node_features_dict[int(line[0])] = np.array(line[1].split(','), dtype=np.uint8)\n",
        "      graph_labels_dict[int(line[0])] = int(line[2])\n",
        "\n",
        "  with open(graph_adjacency_list_file_path, 'r', encoding='utf-8') as graph_adjacency_list_file:\n",
        "    graph_adjacency_list_file.readline()\n",
        "    for line in graph_adjacency_list_file:\n",
        "      line = line.rstrip().split('\\t')\n",
        "      assert (len(line) == 2)\n",
        "      if int(line[0]) not in G:\n",
        "        G.add_node(int(line[0]), features=graph_node_features_dict[int(line[0])],\n",
        "                     label=graph_labels_dict[int(line[0])])\n",
        "      if int(line[1]) not in G:\n",
        "        G.add_node(int(line[1]), features=graph_node_features_dict[int(line[1])],\n",
        "                     label=graph_labels_dict[int(line[1])])\n",
        "      G.add_edge(int(line[0]), int(line[1]))\n",
        "\n",
        "  adj = nx.adjacency_matrix(G, sorted(G.nodes()))\n",
        "  features = np.array(\n",
        "    [features for _, features in sorted(G.nodes(data='features'), key=lambda x: x[0])])\n",
        "  labels = np.array(\n",
        "    [label for _, label in sorted(G.nodes(data='label'), key=lambda x: x[0])])\n",
        "  features = row_normalize(features)\n",
        "\n",
        "  g = adj\n",
        "\n",
        "  with np.load(splits_file_path) as splits_file:\n",
        "      train_mask = splits_file['train_mask']\n",
        "      val_mask = splits_file['val_mask']\n",
        "      test_mask = splits_file['test_mask']\n",
        "\n",
        "  num_features = features.shape[1]\n",
        "  num_labels = len(np.unique(labels))\n",
        "  assert (np.array_equal(np.unique(labels), np.arange(len(np.unique(labels)))))\n",
        "\n",
        "  features = torch.FloatTensor(features)\n",
        "  labels = torch.LongTensor(labels)\n",
        "  train_mask = torch.BoolTensor(train_mask)\n",
        "  val_mask = torch.BoolTensor(val_mask)\n",
        "  test_mask = torch.BoolTensor(test_mask)\n",
        "\n",
        "  g = aug_normalized_adjacency(g)\n",
        "  g = sparse_mx_to_torch_sparse_tensor(g)\n",
        "\n",
        "  return g, features, labels, train_mask, val_mask, test_mask, num_features, num_labels"
      ],
      "metadata": {
        "id": "V2Yr6K6jmi_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extended Train and Test on SGC\n",
        "\n",
        "This requires the `new_data` folder from Geom-GCN project, reference: https://github.com/bingzhewei/geom-gcn/tree/master/new_data and the `splits` folder, reference: https://github.com/bingzhewei/geom-gcn/tree/master/splits.\n",
        "\n",
        "Chamelon dataset is added to the `new_data` folder from GCNII project, reference: https://github.com/chennnM/GCNII/tree/master/new_data/chameleon."
      ],
      "metadata": {
        "id": "PkqWJqKCdI_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#configuration\n",
        "seed = 42 # random seed\n",
        "max_epoch = 100\n",
        "lr = 0.2 # initial learning rate\n",
        "weight_decay=5e-6 # L2 loss on parameters\n",
        "degree = 2 # degrees of approximation\n",
        "\n",
        "# set random and torch seed\n",
        "set_seed(seed)\n",
        "\n",
        "# load the citation data\n",
        "time_acc_data = {}\n",
        "datapath = '/content/drive/MyDrive/colab_notebooks/ece570/project/new_data/'\n",
        "dataset_list = [\"chameleon\", \"cornell\", \"texas\", \"wisconsin\"]\n",
        "splits = []\n",
        "for i in range(10):\n",
        "  splits.append(f'_split_0.6_0.2_{i}.npz')\n",
        "for dataset in dataset_list:\n",
        "  time_acc_data[dataset] = {}\n",
        "  print(f'** SGC training/testing on dataset {dataset}')\n",
        "  for splits_file_name in splits:\n",
        "    split_offset = splits.index(splits_file_name)\n",
        "    time_acc_data[dataset][split_offset] = {}\n",
        "    # train and test SGC\n",
        "    time_list = []\n",
        "    test_acc_list = []\n",
        "    num_loops = 10\n",
        "    for loop in range(num_loops):\n",
        "      loss = 0\n",
        "      train_acc = 0\n",
        "      epoch = 1\n",
        "      adj, features, labels, idx_train, idx_val, idx_test, num_features, num_labels = load_extended_data(datapath, dataset, splits_file_name)\n",
        "      # move to gpu\n",
        "      features = features.to('cuda')\n",
        "      adj = adj.to('cuda')\n",
        "      labels = labels.to('cuda')\n",
        "      idx_train = idx_train.to('cuda')\n",
        "      idx_val = idx_val.to('cuda')\n",
        "      idx_test = idx_test.to('cuda')\n",
        "      # setup model\n",
        "      model = SGC(in_features=features.size(1), out_features=labels.max().item()+1)\n",
        "      model = model.to('cuda')\n",
        "\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "      start = time.time()\n",
        "      # SGC precomputation of features\n",
        "      features = sgc_precompute(features, adj, degree)\n",
        "      for epoch in range(1, max_epoch+1):\n",
        "        loss = loss + train_sgc(model, loss_fn, optimizer, features[idx_train], labels[idx_train], epoch)\n",
        "        train_acc = test_sgc(model, loss_fn, features[idx_val], labels[idx_val], epoch)\n",
        "      test_acc = test_sgc(model, loss_fn, features[idx_test], labels[idx_test], epoch)\n",
        "      end = time.time()\n",
        "      avg_loss = loss/100\n",
        "      train_test_time = end-start\n",
        "      time_list.append(train_test_time)\n",
        "      test_acc_list.append(test_acc.to('cpu'))\n",
        "    time_acc_data[dataset][split_offset][\"mean_time\"] = np.mean(time_list)\n",
        "    time_acc_data[dataset][split_offset][\"test_acc\"] = 100*np.mean(test_acc_list)\n",
        "\n",
        "\n",
        "from tabulate import tabulate\n",
        "table = []\n",
        "header = ['Dataset', 'Split', 'Time (s)', 'Test Accuracy']\n",
        "ds_averages = {}\n",
        "for ds, splits in time_acc_data.items():\n",
        "  ds_total_time = 0.0\n",
        "  ds_total_acc = 0.0\n",
        "  num_splits = len(splits)\n",
        "  for split, data in splits.items():\n",
        "    table.append([ds, split, data['mean_time'], data['test_acc']])\n",
        "    ds_total_time += data['mean_time']\n",
        "    ds_total_acc += data['test_acc']\n",
        "  ds_averages[ds] = {\n",
        "      'mean_time': ds_total_time / num_splits,\n",
        "      'test_acc': ds_total_acc / num_splits\n",
        "  }\n",
        "for ds, avg_data in ds_averages.items():\n",
        "  table.append([ds, 'Average', avg_data['mean_time'], avg_data['test_acc']])\n",
        "table_format = 'grid'\n",
        "print(tabulate(table, headers=header, tablefmt=table_format))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7UuxJ72yLwB",
        "outputId": "d1851cb1-b65f-4259-fed6-9ff967bf0664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** SGC training/testing on dataset chameleon\n",
            "** SGC training/testing on dataset cornell\n",
            "** SGC training/testing on dataset texas\n",
            "** SGC training/testing on dataset wisconsin\n",
            "+-----------+---------+------------+-----------------+\n",
            "| Dataset   | Split   |   Time (s) |   Test Accuracy |\n",
            "+===========+=========+============+=================+\n",
            "| chameleon | 0       |   0.24824  |         61.7982 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 1       |   0.217383 |         63.1579 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 2       |   0.211497 |         60.7456 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 3       |   0.21603  |         60.7895 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 4       |   0.214708 |         60.9649 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 5       |   0.207075 |         64.0351 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 6       |   0.2172   |         59.8684 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 7       |   0.215136 |         58.3333 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 8       |   0.2129   |         63.5526 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 9       |   0.207608 |         62.6535 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 0       |   0.148077 |         54.0541 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 1       |   0.150465 |         56.7568 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 2       |   0.153618 |         59.4595 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 3       |   0.151328 |         45.9459 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 4       |   0.171807 |         54.0541 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 5       |   0.18101  |         64.8649 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 6       |   0.155074 |         54.0541 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 7       |   0.150644 |         64.8649 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 8       |   0.15098  |         56.7568 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 9       |   0.152373 |         54.0541 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 0       |   0.162467 |         51.3514 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 1       |   0.184079 |         54.0541 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 2       |   0.154627 |         51.3514 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 3       |   0.15159  |         59.4595 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 4       |   0.153893 |         48.6486 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 5       |   0.155698 |         56.7568 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 6       |   0.16131  |         51.3514 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 7       |   0.19016  |         48.6486 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 8       |   0.160228 |         56.7568 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 9       |   0.156964 |         70.2703 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 0       |   0.155662 |         41.1765 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 1       |   0.151333 |         47.0588 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 2       |   0.155102 |         43.1373 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 3       |   0.187931 |         45.098  |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 4       |   0.155818 |         45.098  |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 5       |   0.159556 |         52.9412 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 6       |   0.150091 |         47.0588 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 7       |   0.153678 |         52.9412 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 8       |   0.164718 |         50.9804 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 9       |   0.179198 |         45.098  |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | Average |   0.216778 |         61.5899 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | Average |   0.156537 |         56.4865 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | Average |   0.163102 |         54.8649 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | Average |   0.161309 |         47.0588 |\n",
            "+-----------+---------+------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extended Train and Test on GCN\n",
        "\n",
        "This requires the `new_data` folder from Geom-GCN project, reference: https://github.com/bingzhewei/geom-gcn/tree/master/new_data and the `splits` folder, reference: https://github.com/bingzhewei/geom-gcn/tree/master/splits."
      ],
      "metadata": {
        "id": "h-feHkC0dNxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configuration from pygcn\n",
        "seed = 42 # random seed\n",
        "max_epoch = 100\n",
        "lr = 0.01 # initial learning rate\n",
        "weight_decay=5e-4 # L2 loss on parameters\n",
        "hidden = 50 # number of hidden units\n",
        "dropout = 0.5 # dropout rate (1 - keep probability)\n",
        "\n",
        "# set random and torch seed\n",
        "set_seed(seed)\n",
        "\n",
        "# load the citation data\n",
        "time_acc_data = {}\n",
        "datapath = '/content/drive/MyDrive/colab_notebooks/ece570/project/new_data/'\n",
        "dataset_list = [\"chameleon\", \"cornell\", \"texas\", \"wisconsin\"]\n",
        "splits = []\n",
        "for i in range(10):\n",
        "  splits.append(f'_split_0.6_0.2_{i}.npz')\n",
        "for dataset in dataset_list:\n",
        "  time_acc_data[dataset] = {}\n",
        "  print(f'** GCN training/testing on dataset {dataset} **')\n",
        "  for splits_file_name in splits:\n",
        "    split_offset = splits.index(splits_file_name)\n",
        "    time_acc_data[dataset][split_offset] = {}\n",
        "    # train and test SGC\n",
        "    time_list = []\n",
        "    test_acc_list = []\n",
        "    num_loops = 10\n",
        "    for loop in range(num_loops):\n",
        "      loss = 0\n",
        "      train_acc = 0\n",
        "      epoch = 1\n",
        "      adj, features, labels, idx_train, idx_val, idx_test, num_features, num_labels = load_extended_data(datapath, dataset, splits_file_name)\n",
        "      # move to gpu\n",
        "      features = features.to('cuda')\n",
        "      adj = adj.to('cuda')\n",
        "      labels = labels.to('cuda')\n",
        "      idx_train = idx_train.to('cuda')\n",
        "      idx_val = idx_val.to('cuda')\n",
        "      idx_test = idx_test.to('cuda')\n",
        "      # setup model\n",
        "      model = GCN(in_features=features.shape[1], in_hidden=hidden, out_features=labels.max().item()+1, dropout=dropout)\n",
        "      model = model.to('cuda')\n",
        "      # setup optimizer\n",
        "      optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "      # train and test GCN\n",
        "      start = time.time()\n",
        "      for epoch in range(1, max_epoch+1):\n",
        "        loss = loss + train_gcn(model, optimizer, features, labels, adj, idx_train, idx_val, epoch)\n",
        "        train_acc = test_gcn(model, features, labels, adj, idx_test, epoch)\n",
        "      test_acc = test_gcn(model, features, labels, adj, idx_test, epoch)\n",
        "      end = time.time()\n",
        "      avg_loss = loss/100\n",
        "      train_test_time = end-start\n",
        "      time_list.append(train_test_time)\n",
        "      test_acc_list.append(test_acc.to('cpu'))\n",
        "    time_acc_data[dataset][split_offset][\"mean_time\"] = np.mean(time_list)\n",
        "    time_acc_data[dataset][split_offset][\"test_acc\"] = 100*np.mean(test_acc_list)\n",
        "\n",
        "\n",
        "from tabulate import tabulate\n",
        "table = []\n",
        "header = ['Dataset', 'Split', 'Time (s)', 'Test Accuracy']\n",
        "ds_averages = {}\n",
        "for ds, splits in time_acc_data.items():\n",
        "  ds_total_time = 0.0\n",
        "  ds_total_acc = 0.0\n",
        "  num_splits = len(splits)\n",
        "  for split, data in splits.items():\n",
        "    table.append([ds, split, data['mean_time'], data['test_acc']])\n",
        "    ds_total_time += data['mean_time']\n",
        "    ds_total_acc += data['test_acc']\n",
        "  ds_averages[ds] = {\n",
        "      'mean_time': ds_total_time / num_splits,\n",
        "      'test_acc': ds_total_acc / num_splits\n",
        "  }\n",
        "for ds, avg_data in ds_averages.items():\n",
        "  table.append([ds, 'Average', avg_data['mean_time'], avg_data['test_acc']])\n",
        "table_format = 'grid'\n",
        "print(tabulate(table, headers=header, tablefmt=table_format))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeVXil1qdQGj",
        "outputId": "7d3fde19-81dd-4fd4-c8da-54bee3e9ce22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** GCN training/testing on dataset chameleon **\n",
            "** GCN training/testing on dataset cornell **\n",
            "** GCN training/testing on dataset texas **\n",
            "** GCN training/testing on dataset wisconsin **\n",
            "+-----------+---------+------------+-----------------+\n",
            "| Dataset   | Split   |   Time (s) |   Test Accuracy |\n",
            "+===========+=========+============+=================+\n",
            "| chameleon | 0       |   0.571254 |         55.6579 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 1       |   0.582402 |         59.2105 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 2       |   0.601417 |         53.3772 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 3       |   0.599092 |         59.5614 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 4       |   0.577061 |         55.7237 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 5       |   0.585506 |         57.0175 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 6       |   0.580515 |         52.2149 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 7       |   0.580106 |         53.6184 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 8       |   0.570811 |         55.6798 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 9       |   0.566698 |         56.8202 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 0       |   0.577581 |         59.1892 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 1       |   0.511945 |         60.5405 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 2       |   0.588175 |         62.7027 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 3       |   0.519938 |         52.4324 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 4       |   0.562761 |         61.3514 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 5       |   0.523209 |         64.5946 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 6       |   0.526944 |         57.8378 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 7       |   0.5699   |         57.8378 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 8       |   0.504106 |         60      |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 9       |   0.585901 |         54.8649 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 0       |   0.509519 |         59.4595 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 1       |   0.589508 |         48.9189 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 2       |   0.507719 |         47.027  |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 3       |   0.539912 |         62.973  |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 4       |   0.528028 |         50.5405 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 5       |   0.513834 |         47.027  |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 6       |   0.561646 |         48.6486 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 7       |   0.493573 |         44.5946 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 8       |   0.574376 |         52.4324 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 9       |   0.516188 |         66.2162 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 0       |   0.579988 |         41.9608 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 1       |   0.511073 |         51.7647 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 2       |   0.565094 |         49.2157 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 3       |   0.529411 |         44.902  |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 4       |   0.543509 |         45.2941 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 5       |   0.543589 |         48.6275 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 6       |   0.500325 |         49.4118 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 7       |   0.578728 |         49.4118 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 8       |   0.512239 |         54.3137 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 9       |   0.581582 |         47.2549 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | Average |   0.581486 |         55.8882 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | Average |   0.547046 |         59.1351 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | Average |   0.53343  |         52.7838 |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | Average |   0.544554 |         48.2157 |\n",
            "+-----------+---------+------------+-----------------+\n"
          ]
        }
      ]
    }
  ]
}