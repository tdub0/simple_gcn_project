{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Global Dependencies"
      ],
      "metadata": {
        "id": "NUZYIB2tlIVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "from typing import List, Dict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\n",
        "    \"ignore\",\n",
        "    category=DeprecationWarning,\n",
        ")"
      ],
      "metadata": {
        "id": "UqVgdpa7lHkT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Mount Datasets\n",
        "The datasets are mounted using Google Drive."
      ],
      "metadata": {
        "id": "y4LP_5Olq6xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twU2Bu4G00RO",
        "outputId": "362874c1-fe63-4dbc-f0fa-8bd70790630e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Random Seed"
      ],
      "metadata": {
        "id": "sJNP-OBtScSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "szna4OhfSbtE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Models\n",
        "\n",
        "GCN - Graph Convolutional Network uses two graph convolution layers and a dropout layer.\n",
        "\n",
        "SGC - Simple Graph Convolution Network using a simple PyTorch implementation of logistic regression.\n",
        "\n",
        "Assumption of model SGC model use:\n",
        "- Features have been preprocessed with k-step graph propagation"
      ],
      "metadata": {
        "id": "zbHxKOABkjK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGC(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(SGC, self).__init__()\n",
        "        # setup as fully connected network\n",
        "        self.fc1 = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # forward feed the data through the fc layer\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GCL(nn.modules.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(GCL, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.parameter.Parameter(\n",
        "            torch.FloatTensor(in_features, out_features)\n",
        "        )\n",
        "        self.bias = nn.parameter.Parameter(torch.FloatTensor(out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        # forward feed the data through the fc layer\n",
        "        support = torch.mm(x, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        return output + self.bias\n",
        "\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_features, in_hidden, out_features, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "        self.gc1 = GCL(in_features, in_hidden)\n",
        "        self.gc2 = GCL(in_hidden, out_features)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = self.gc1(x, adj)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "Ob1zkz23kf8w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Normalization\n",
        "\n",
        "Graphs should be normalized.\n",
        "\n",
        "Reference: https://github.com/Tiiiger/SGC/blob/master/normalization.py"
      ],
      "metadata": {
        "id": "RI0IrGhupfgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64)\n",
        "    )\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse_coo_tensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def aug_normalized_adjacency(adj):\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    adj = adj + sp.eye(adj.shape[0])\n",
        "    row_sum = np.array(adj.sum(1))\n",
        "    row_sum = (row_sum == 0) * 1 + row_sum\n",
        "    d_inv_sqrt = np.power(row_sum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt).tocoo()\n",
        "\n",
        "\n",
        "def row_normalize(mx):\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    rowsum = (rowsum == 0) * 1 + rowsum\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.0\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx"
      ],
      "metadata": {
        "id": "3M47Z13rppE-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Analysis\n",
        "\n",
        "Reference: https://github.com/Tiiiger/SGC/blob/master/metrics.py"
      ],
      "metadata": {
        "id": "3BTm6i0vr6t0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(output, labels):\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)"
      ],
      "metadata": {
        "id": "lxYWJod_r95o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Loading Datasets\n",
        "\n",
        "Modified from the reference: https://github.com/Tiiiger/SGC/blob/master/utils.py\n",
        "\n",
        "Added randomized splits."
      ],
      "metadata": {
        "id": "kb4S7c5FsP8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_index_file(filename):\n",
        "    index = []\n",
        "    for line in open(filename):\n",
        "        index.append(int(line.strip()))\n",
        "    return index\n",
        "\n",
        "\n",
        "def sgc_precompute(features, adj, degree):\n",
        "    for i in range(degree):\n",
        "        features = torch.mm(adj, features)\n",
        "    return features"
      ],
      "metadata": {
        "id": "IJ7CT6NKBMQu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_citation_sgc(path, dataset_str=\"cora\"):\n",
        "    \"\"\"Load Citation Networks Datasets\"\"\"\n",
        "    import pickle as pkl\n",
        "    import networkx as nx\n",
        "\n",
        "    names = [\"x\", \"y\", \"tx\", \"ty\", \"allx\", \"ally\", \"graph\"]\n",
        "    objects = []\n",
        "    for i in range(len(names)):\n",
        "        with open(\n",
        "            \"{}/ind.{}.{}\".format(path, dataset_str.lower(), names[i]), \"rb\"\n",
        "        ) as f:\n",
        "            objects.append(pkl.load(f, encoding=\"latin1\"))\n",
        "\n",
        "    x, y, tx, ty, all_x, all_y, graph = tuple(objects)\n",
        "    test_idx_reorder = parse_index_file(\n",
        "        \"{}/ind.{}.test.index\".format(path, dataset_str)\n",
        "    )\n",
        "    test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "    features = sp.vstack((all_x, tx)).tolil()\n",
        "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "\n",
        "    labels = np.vstack((all_y, ty))\n",
        "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "    idx_test = test_idx_range.tolist()  # range(500,1500)\n",
        "    idx_train = range(len(y))  # range(140)\n",
        "    idx_val = range(len(y), len(y) + 500)  # range(200,500)\n",
        "\n",
        "    features = row_normalize(features)\n",
        "\n",
        "    # porting to pytorch\n",
        "    features = torch.FloatTensor(np.array(features.todense())).float()\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = aug_normalized_adjacency(adj)\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_val, idx_test"
      ],
      "metadata": {
        "id": "5aElu8L8sYmR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Train & Test Functions"
      ],
      "metadata": {
        "id": "2y8ZvQrsuDSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sgc(model, loss_fn, optimizer, train_features, train_labels):\n",
        "    # set the mode for the model\n",
        "    model.train()\n",
        "    # init optimizer as gradients accumulate\n",
        "    optimizer.zero_grad()\n",
        "    # feed forward the data into our network\n",
        "    output = model(train_features)\n",
        "    # compute the loss between the output of\n",
        "    # network and actual label\n",
        "    loss = loss_fn(output, train_labels)\n",
        "    # backwards propagation\n",
        "    loss.backward()\n",
        "    # move a step on the gradient by optimizer\n",
        "    optimizer.step()\n",
        "    # record the loss\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def test_sgc(model, loss_fn, test_features, test_labels):\n",
        "    # set the model mode to test\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(test_features)\n",
        "        acc_val = accuracy(output, test_labels)\n",
        "    return acc_val\n",
        "\n",
        "\n",
        "def train_gcn(model, optimizer, features, labels, adj, idx_train, idx_val):\n",
        "    # set the mode for the model\n",
        "    model.train()\n",
        "    # init optimizer as gradients accumulate\n",
        "    optimizer.zero_grad()\n",
        "    # feed forward the data into our network\n",
        "    output = model(features, adj)\n",
        "    # compute the loss between the output of\n",
        "    # network and actual label\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    # backwards propagation\n",
        "    loss_train.backward()\n",
        "    # move a step on the gradient by optimizer\n",
        "    optimizer.step()\n",
        "    # record the loss\n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    return loss_val.item()\n",
        "\n",
        "\n",
        "def test_gcn(model, features, labels, adj, idx_test):\n",
        "    # set the model mode to test\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(features, adj)\n",
        "        acc_val = accuracy(output[idx_test], labels[idx_test])\n",
        "    return acc_val"
      ],
      "metadata": {
        "id": "P1WJ8HpnuJaY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Results Handling\n",
        "\n",
        "The following function prints out the train and test results in an easy to read table."
      ],
      "metadata": {
        "id": "nLOM5Zsr_Gw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "def print_results_as_table(results_dict):\n",
        "    table = []\n",
        "    header = [\"Dataset\", \"Split\", \"Time (s)\", \"Test Accuracy\"]\n",
        "    ds_averages = {}\n",
        "    for ds, splits in results_dict.items():\n",
        "        ds_total_time = 0.0\n",
        "        ds_total_acc = 0.0\n",
        "        ds_total_std = 0.0\n",
        "        num_splits = len(splits)\n",
        "        for split, data in splits.items():\n",
        "            table.append([ds, split, f\"{data['mean_time']:.2f}\", f\"{data['test_acc']:.1f} +/- {data['test_std_dev']:.1f}\"])\n",
        "            ds_total_time += data[\"mean_time\"]\n",
        "            ds_total_acc += data[\"test_acc\"]\n",
        "            ds_total_std += data[\"test_std_dev\"]\n",
        "        ds_averages[ds] = {\n",
        "            \"mean_time\": f\"{ds_total_time / num_splits:.2f}\",\n",
        "            \"test_acc\": f\"{ds_total_acc / num_splits:.1f} +/- {ds_total_std / num_splits:.1f}\"\n",
        "        }\n",
        "    for ds, avg_data in ds_averages.items():\n",
        "        table.append([ds, \"Average\", avg_data[\"mean_time\"], avg_data[\"test_acc\"]])\n",
        "    table_format = \"grid\"\n",
        "    print(tabulate(table, headers=header, tablefmt=table_format))"
      ],
      "metadata": {
        "id": "QDSrnNGU-jtA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Citation Network Testing for SGC and GCN\n",
        "This requires the `sgcn_data` folder with the `Cora` and `Pubmed` datasets. `sgcn_data` is just the `data` folder from SGC, reference: https://github.com/Tiiiger/SGC/tree/master/data."
      ],
      "metadata": {
        "id": "GX8yJtXgyaSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configuration\n",
        "seed = 42  # random seed\n",
        "max_epoch = 100\n",
        "lr = 0.2  # initial learning rate\n",
        "weight_decay = 5e-6  # L2 loss on parameters\n",
        "degree = 2  # degrees of approximation\n",
        "\n",
        "# set random and torch seed\n",
        "set_seed(seed)\n",
        "\n",
        "# setup results dictionary\n",
        "results_dict = {}\n",
        "split = \"N/A\"\n",
        "# number of loops to average\n",
        "num_loops = 20\n",
        "\n",
        "# load the citation data\n",
        "datapath = \"/content/drive/MyDrive/sgcn_data/\"\n",
        "dataset_list = [\"cora\", \"pubmed\"]\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    results_dict[dataset] = {}\n",
        "    results_dict[dataset][split] = {}\n",
        "    print(f\"** SGC training/testing on dataset {dataset} **\")\n",
        "    time_list = []\n",
        "    test_acc_list = []\n",
        "    for loop in range(num_loops):\n",
        "        loss = 0\n",
        "        train_acc = 0\n",
        "        epoch = 1\n",
        "        adj, features, labels, idx_train, idx_val, idx_test = load_citation_sgc(\n",
        "            datapath, dataset\n",
        "        )\n",
        "        # move to gpu\n",
        "        features = features.to(\"cuda\")\n",
        "        adj = adj.to(\"cuda\")\n",
        "        labels = labels.to(\"cuda\")\n",
        "        idx_train = idx_train.to(\"cuda\")\n",
        "        idx_val = idx_val.to(\"cuda\")\n",
        "        idx_test = idx_test.to(\"cuda\")\n",
        "        # setup model\n",
        "        model = SGC(in_features=features.size(1), out_features=labels.max().item() + 1)\n",
        "        model = model.to(\"cuda\")\n",
        "        # setup loss and optimizer\n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        # start training time\n",
        "        start = time.time()\n",
        "        # SGC precomputation of features\n",
        "        features = sgc_precompute(features, adj, degree)\n",
        "        # train and validate for the number of epochs\n",
        "        for epoch in range(1, max_epoch + 1):\n",
        "            loss = loss + train_sgc(\n",
        "                model, loss_fn, optimizer, features[idx_train], labels[idx_train]\n",
        "            )\n",
        "            train_acc = test_sgc(\n",
        "                model, loss_fn, features[idx_val], labels[idx_val]\n",
        "            )\n",
        "        # end training time\n",
        "        end = time.time()\n",
        "        # measure test accuracy\n",
        "        test_acc = test_sgc(model, loss_fn, features[idx_test], labels[idx_test])\n",
        "        avg_loss = loss / 100\n",
        "        train_time = end - start\n",
        "        time_list.append(train_time)\n",
        "        test_acc_list.append(test_acc.to(\"cpu\"))\n",
        "    results_dict[dataset][split][\"mean_time\"] = np.mean(time_list)\n",
        "    results_dict[dataset][split][\"test_acc\"] = 100 * np.mean(test_acc_list)\n",
        "    results_dict[dataset][split][\"test_std_dev\"] = 100 * np.std(test_acc_list)\n",
        "\n",
        "print_results_as_table(results_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n-gRtYVydFr",
        "outputId": "5587c879-98f2-4f80-fc6d-497627a7e465"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** SGC training/testing on dataset cora **\n",
            "** SGC training/testing on dataset pubmed **\n",
            "+-----------+---------+------------+-----------------+\n",
            "| Dataset   | Split   |   Time (s) | Test Accuracy   |\n",
            "+===========+=========+============+=================+\n",
            "| cora      | N/A     |       0.16 | 80.6 +/- 0.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| pubmed    | N/A     |       0.14 | 77.9 +/- 0.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cora      | Average |       0.16 | 80.6 +/- 0.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| pubmed    | Average |       0.14 | 77.9 +/- 0.1    |\n",
            "+-----------+---------+------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configuration from pygcn\n",
        "seed = 42  # random seed\n",
        "max_epoch = 100\n",
        "lr = 0.01  # initial learning rate\n",
        "weight_decay = 5e-4  # L2 loss on parameters\n",
        "hidden = 50  # number of hidden units\n",
        "dropout = 0.5  # dropout rate (1 - keep probability)\n",
        "\n",
        "# set random and torch seed\n",
        "set_seed(seed)\n",
        "\n",
        "# setup results dictionary\n",
        "results_dict = {}\n",
        "split = \"N/A\"\n",
        "# number of loops to average\n",
        "num_loops = 20\n",
        "\n",
        "# load the citation data\n",
        "datapath = \"/content/drive/MyDrive/sgcn_data/\"\n",
        "dataset_list = [\"cora\", \"pubmed\"]\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    results_dict[dataset] = {}\n",
        "    results_dict[dataset][split] = {}\n",
        "    print(f\"** GCN training/testing on dataset {dataset} **\")\n",
        "    time_list = []\n",
        "    test_acc_list = []\n",
        "    for loop in range(num_loops):\n",
        "        loss = 0\n",
        "        train_acc = 0\n",
        "        epoch = 1\n",
        "        adj, features, labels, idx_train, idx_val, idx_test = load_citation_sgc(\n",
        "            datapath, dataset\n",
        "        )\n",
        "        # move to gpu\n",
        "        features = features.to(\"cuda\")\n",
        "        adj = adj.to(\"cuda\")\n",
        "        labels = labels.to(\"cuda\")\n",
        "        idx_train = idx_train.to(\"cuda\")\n",
        "        idx_val = idx_val.to(\"cuda\")\n",
        "        idx_test = idx_test.to(\"cuda\")\n",
        "        # setup model\n",
        "        model = GCN(\n",
        "            in_features=features.shape[1],\n",
        "            in_hidden=hidden,\n",
        "            out_features=labels.max().item() + 1,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        model = model.to(\"cuda\")\n",
        "        # setup optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        # start training time\n",
        "        start = time.time()\n",
        "        # train and validate GCN\n",
        "        for epoch in range(1, max_epoch + 1):\n",
        "            loss = loss + train_gcn(\n",
        "                model, optimizer, features, labels, adj, idx_train, idx_val\n",
        "            )\n",
        "            train_acc = test_gcn(model, features, labels, adj, idx_test)\n",
        "        # end training time\n",
        "        end = time.time()\n",
        "        # measure test accuracy\n",
        "        test_acc = test_gcn(model, features, labels, adj, idx_test)\n",
        "        avg_loss = loss / 100\n",
        "        train_test_time = end - start\n",
        "        time_list.append(train_test_time)\n",
        "        test_acc_list.append(test_acc.to(\"cpu\"))\n",
        "    results_dict[dataset][split][\"mean_time\"] = np.mean(time_list)\n",
        "    results_dict[dataset][split][\"test_acc\"] = 100 * np.mean(test_acc_list)\n",
        "    results_dict[dataset][split][\"test_std_dev\"] = 100 * np.std(test_acc_list)\n",
        "\n",
        "print_results_as_table(results_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsd1xFVATJuc",
        "outputId": "4df1e035-1275-43bc-88a1-04f8632426bb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** GCN training/testing on dataset cora **\n",
            "** GCN training/testing on dataset pubmed **\n",
            "+-----------+---------+------------+-----------------+\n",
            "| Dataset   | Split   |   Time (s) | Test Accuracy   |\n",
            "+===========+=========+============+=================+\n",
            "| cora      | N/A     |       0.58 | 81.2 +/- 0.4    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| pubmed    | N/A     |       0.71 | 79.3 +/- 0.4    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cora      | Average |       0.58 | 81.2 +/- 0.4    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| pubmed    | Average |       0.71 | 79.3 +/- 0.4    |\n",
            "+-----------+---------+------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11. Extended Dataset Loading\n",
        "\n",
        "Running SGC to compare against extended datasets from Geom-GCN and GCNII.\n",
        "\n",
        "Modified from reference: https://github.com/bingzhewei/geom-gcn/blob/master/utils_data.py"
      ],
      "metadata": {
        "id": "dJuhRNFgmjxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_extended_data(path, dataset_name, splits_file_name):\n",
        "    graph_adjacency_list_file_path = path + dataset_name + \"/out1_graph_edges.txt\"\n",
        "    graph_node_features_and_labels_file_path = (\n",
        "        path + dataset_name + \"/out1_node_feature_label.txt\"\n",
        "    )\n",
        "\n",
        "    import networkx as nx\n",
        "\n",
        "    G = nx.DiGraph()\n",
        "    graph_node_features_dict = {}\n",
        "    graph_labels_dict = {}\n",
        "\n",
        "    with open(\n",
        "        graph_node_features_and_labels_file_path, \"r\", encoding=\"utf-8\"\n",
        "    ) as graph_node_features_and_labels_file:\n",
        "        graph_node_features_and_labels_file.readline()\n",
        "        for line in graph_node_features_and_labels_file:\n",
        "            line = line.rstrip().split(\"\\t\")\n",
        "            assert len(line) == 3\n",
        "            assert (\n",
        "                int(line[0]) not in graph_node_features_dict\n",
        "                and int(line[0]) not in graph_labels_dict\n",
        "            )\n",
        "            graph_node_features_dict[int(line[0])] = np.array(\n",
        "                line[1].split(\",\"), dtype=np.uint8\n",
        "            )\n",
        "            graph_labels_dict[int(line[0])] = int(line[2])\n",
        "\n",
        "    with open(\n",
        "        graph_adjacency_list_file_path, \"r\", encoding=\"utf-8\"\n",
        "    ) as graph_adjacency_list_file:\n",
        "        graph_adjacency_list_file.readline()\n",
        "        for line in graph_adjacency_list_file:\n",
        "            line = line.rstrip().split(\"\\t\")\n",
        "            assert len(line) == 2\n",
        "            if int(line[0]) not in G:\n",
        "                G.add_node(\n",
        "                    int(line[0]),\n",
        "                    features=graph_node_features_dict[int(line[0])],\n",
        "                    label=graph_labels_dict[int(line[0])],\n",
        "                )\n",
        "            if int(line[1]) not in G:\n",
        "                G.add_node(\n",
        "                    int(line[1]),\n",
        "                    features=graph_node_features_dict[int(line[1])],\n",
        "                    label=graph_labels_dict[int(line[1])],\n",
        "                )\n",
        "            G.add_edge(int(line[0]), int(line[1]))\n",
        "\n",
        "    adj = nx.adjacency_matrix(G, sorted(G.nodes()))\n",
        "    features = np.array(\n",
        "        [\n",
        "            features\n",
        "            for _, features in sorted(G.nodes(data=\"features\"), key=lambda x: x[0])\n",
        "        ]\n",
        "    )\n",
        "    labels = np.array(\n",
        "        [label for _, label in sorted(G.nodes(data=\"label\"), key=lambda x: x[0])]\n",
        "    )\n",
        "    features = row_normalize(features)\n",
        "\n",
        "    g = adj\n",
        "\n",
        "    splits_file_path = path + \"splits/\" + dataset_name + splits_file_name\n",
        "    with np.load(splits_file_path) as splits_file:\n",
        "        train_mask = splits_file[\"train_mask\"]\n",
        "        val_mask = splits_file[\"val_mask\"]\n",
        "        test_mask = splits_file[\"test_mask\"]\n",
        "\n",
        "    num_features = features.shape[1]\n",
        "    num_labels = len(np.unique(labels))\n",
        "    assert np.array_equal(np.unique(labels), np.arange(len(np.unique(labels))))\n",
        "\n",
        "    features = torch.FloatTensor(features)\n",
        "    labels = torch.LongTensor(labels)\n",
        "    train_mask = torch.BoolTensor(train_mask)\n",
        "    val_mask = torch.BoolTensor(val_mask)\n",
        "    test_mask = torch.BoolTensor(test_mask)\n",
        "\n",
        "    g = aug_normalized_adjacency(g)\n",
        "    g = sparse_mx_to_torch_sparse_tensor(g)\n",
        "\n",
        "    return g, features, labels, train_mask, val_mask, test_mask"
      ],
      "metadata": {
        "id": "V2Yr6K6jmi_I"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Extended Dataset Testing on SGC\n",
        "\n",
        "This requires the `new_data` folder from Geom-GCN project, reference: https://github.com/bingzhewei/geom-gcn/tree/master/new_data and the `splits` folder, reference: https://github.com/bingzhewei/geom-gcn/tree/master/splits.\n",
        "\n",
        "Chamelon dataset is added to the `new_data` folder from GCNII project, reference: https://github.com/chennnM/GCNII/tree/master/new_data/chameleon."
      ],
      "metadata": {
        "id": "PkqWJqKCdI_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configuration\n",
        "seed = 42  # random seed\n",
        "max_epoch = 100\n",
        "lr = 0.2  # initial learning rate\n",
        "weight_decay = 5e-6  # L2 loss on parameters\n",
        "degree = 2  # degrees of approximation\n",
        "\n",
        "# set random and torch seed\n",
        "set_seed(seed)\n",
        "\n",
        "# setup results dictionary\n",
        "results_dict = {}\n",
        "# number of loops to average\n",
        "num_loops = 20\n",
        "\n",
        "# load the new datasets\n",
        "datapath = \"/content/drive/MyDrive/new_data/\"\n",
        "dataset_list = [\"chameleon\", \"cornell\", \"texas\", \"wisconsin\"]\n",
        "\n",
        "# load the randomized splits\n",
        "splits = []\n",
        "for i in range(10):\n",
        "    splits.append(f\"_split_0.6_0.2_{i}.npz\")\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    results_dict[dataset] = {}\n",
        "    print(f\"** SGC training/testing on dataset {dataset}\")\n",
        "    for splits_file_name in splits:\n",
        "        split_offset = splits.index(splits_file_name)\n",
        "        results_dict[dataset][split_offset] = {}\n",
        "        # train and test SGC\n",
        "        time_list = []\n",
        "        test_acc_list = []\n",
        "        for loop in range(num_loops):\n",
        "            loss = 0\n",
        "            train_acc = 0\n",
        "            epoch = 1\n",
        "            adj, features, labels, train_mask, val_mask, test_mask = load_extended_data(\n",
        "                datapath, dataset, splits_file_name\n",
        "            )\n",
        "            # move to gpu\n",
        "            features = features.to(\"cuda\")\n",
        "            adj = adj.to(\"cuda\")\n",
        "            labels = labels.to(\"cuda\")\n",
        "            train_mask = train_mask.to(\"cuda\")\n",
        "            val_mask = val_mask.to(\"cuda\")\n",
        "            test_mask = test_mask.to(\"cuda\")\n",
        "            # setup model\n",
        "            model = SGC(\n",
        "                in_features=features.size(1), out_features=labels.max().item() + 1\n",
        "            )\n",
        "            model = model.to(\"cuda\")\n",
        "\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "            start = time.time()\n",
        "            # SGC precomputation of features\n",
        "            features = sgc_precompute(features, adj, degree)\n",
        "            for epoch in range(1, max_epoch + 1):\n",
        "                loss = loss + train_sgc(\n",
        "                    model,\n",
        "                    loss_fn,\n",
        "                    optimizer,\n",
        "                    features[train_mask],\n",
        "                    labels[train_mask]\n",
        "                )\n",
        "                train_acc = test_sgc(\n",
        "                    model, loss_fn, features[val_mask], labels[val_mask]\n",
        "                )\n",
        "            test_acc = test_sgc(\n",
        "                model, loss_fn, features[test_mask], labels[test_mask]\n",
        "            )\n",
        "            end = time.time()\n",
        "            avg_loss = loss / 100\n",
        "            train_test_time = end - start\n",
        "            time_list.append(train_test_time)\n",
        "            test_acc_list.append(test_acc.to(\"cpu\"))\n",
        "        results_dict[dataset][split_offset][\"mean_time\"] = np.mean(time_list)\n",
        "        results_dict[dataset][split_offset][\"test_acc\"] = 100 * np.mean(test_acc_list)\n",
        "        results_dict[dataset][split_offset][\"test_std_dev\"] = 100 * np.std(test_acc_list)\n",
        "\n",
        "print_results_as_table(results_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7UuxJ72yLwB",
        "outputId": "263703c3-b8b4-4acc-af83-214c004585cb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** SGC training/testing on dataset chameleon\n",
            "** SGC training/testing on dataset cornell\n",
            "** SGC training/testing on dataset texas\n",
            "** SGC training/testing on dataset wisconsin\n",
            "+-----------+---------+------------+-----------------+\n",
            "| Dataset   | Split   |   Time (s) | Test Accuracy   |\n",
            "+===========+=========+============+=================+\n",
            "| chameleon | 0       |       0.24 | 61.8 +/- 0.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 1       |       0.2  | 63.2 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 2       |       0.2  | 60.7 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 3       |       0.2  | 60.8 +/- 0.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 4       |       0.2  | 61.0 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 5       |       0.21 | 64.0 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 6       |       0.2  | 59.8 +/- 0.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 7       |       0.2  | 58.3 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 8       |       0.21 | 63.6 +/- 0.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 9       |       0.2  | 62.7 +/- 0.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 0       |       0.16 | 54.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 1       |       0.14 | 56.8 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 2       |       0.15 | 59.5 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 3       |       0.16 | 45.9 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 4       |       0.14 | 54.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 5       |       0.19 | 64.9 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 6       |       0.17 | 54.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 7       |       0.15 | 64.9 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 8       |       0.14 | 56.8 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 9       |       0.17 | 54.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 0       |       0.14 | 51.4 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 1       |       0.15 | 54.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 2       |       0.17 | 51.4 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 3       |       0.15 | 59.5 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 4       |       0.14 | 48.6 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 5       |       0.16 | 56.8 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 6       |       0.16 | 51.4 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 7       |       0.15 | 48.6 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 8       |       0.15 | 56.8 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 9       |       0.18 | 70.3 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 0       |       0.15 | 41.2 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 1       |       0.15 | 47.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 2       |       0.17 | 43.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 3       |       0.15 | 45.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 4       |       0.14 | 45.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 5       |       0.18 | 52.9 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 6       |       0.15 | 47.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 7       |       0.15 | 52.9 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 8       |       0.17 | 51.0 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 9       |       0.15 | 45.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | Average |       0.21 | 61.6 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | Average |       0.16 | 56.5 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | Average |       0.16 | 54.9 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | Average |       0.15 | 47.1 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13. Extended Dataset Testing on GCN\n",
        "\n",
        "This requires the `new_data` folder from Geom-GCN project, reference: https://github.com/bingzhewei/geom-gcn/tree/master/new_data and the `splits` folder, reference: https://github.com/bingzhewei/geom-gcn/tree/master/splits."
      ],
      "metadata": {
        "id": "h-feHkC0dNxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# configuration from pygcn\n",
        "seed = 42  # random seed\n",
        "max_epoch = 100\n",
        "lr = 0.01  # initial learning rate\n",
        "weight_decay = 5e-4  # L2 loss on parameters\n",
        "hidden = 50  # number of hidden units\n",
        "dropout = 0.5  # dropout rate (1 - keep probability)\n",
        "\n",
        "# set random and torch seed\n",
        "set_seed(seed)\n",
        "\n",
        "# setup results dictionary\n",
        "results_dict = {}\n",
        "# number of loops to average\n",
        "num_loops = 20\n",
        "\n",
        "# load the citation data\n",
        "datapath = \"/content/drive/MyDrive/new_data/\"\n",
        "dataset_list = [\"chameleon\", \"cornell\", \"texas\", \"wisconsin\"]\n",
        "\n",
        "# load the randomized splits\n",
        "splits = []\n",
        "for i in range(10):\n",
        "    splits.append(f\"_split_0.6_0.2_{i}.npz\")\n",
        "\n",
        "for dataset in dataset_list:\n",
        "    results_dict[dataset] = {}\n",
        "    print(f\"** GCN training/testing on dataset {dataset} **\")\n",
        "    for splits_file_name in splits:\n",
        "        split_offset = splits.index(splits_file_name)\n",
        "        results_dict[dataset][split_offset] = {}\n",
        "        # train and test SGC\n",
        "        time_list = []\n",
        "        test_acc_list = []\n",
        "        for loop in range(num_loops):\n",
        "            loss = 0\n",
        "            train_acc = 0\n",
        "            epoch = 1\n",
        "            adj, features, labels, idx_train, idx_val, idx_test = load_extended_data(\n",
        "                datapath, dataset, splits_file_name\n",
        "            )\n",
        "            # move to gpu\n",
        "            features = features.to(\"cuda\")\n",
        "            adj = adj.to(\"cuda\")\n",
        "            labels = labels.to(\"cuda\")\n",
        "            idx_train = idx_train.to(\"cuda\")\n",
        "            idx_val = idx_val.to(\"cuda\")\n",
        "            idx_test = idx_test.to(\"cuda\")\n",
        "            # setup model\n",
        "            model = GCN(\n",
        "                in_features=features.shape[1],\n",
        "                in_hidden=hidden,\n",
        "                out_features=labels.max().item() + 1,\n",
        "                dropout=dropout,\n",
        "            )\n",
        "            model = model.to(\"cuda\")\n",
        "            # setup optimizer\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "            # train and test GCN\n",
        "            start = time.time()\n",
        "            for epoch in range(1, max_epoch + 1):\n",
        "                loss = loss + train_gcn(\n",
        "                    model, optimizer, features, labels, adj, idx_train, idx_val\n",
        "                )\n",
        "                train_acc = test_gcn(model, features, labels, adj, idx_test)\n",
        "            test_acc = test_gcn(model, features, labels, adj, idx_test)\n",
        "            end = time.time()\n",
        "            avg_loss = loss / 100\n",
        "            train_test_time = end - start\n",
        "            time_list.append(train_test_time)\n",
        "            test_acc_list.append(test_acc.to(\"cpu\"))\n",
        "        results_dict[dataset][split_offset][\"mean_time\"] = np.mean(time_list)\n",
        "        results_dict[dataset][split_offset][\"test_acc\"] = 100 * np.mean(test_acc_list)\n",
        "        results_dict[dataset][split_offset][\"test_std_dev\"] = 100 * np.std(test_acc_list)\n",
        "\n",
        "print_results_as_table(results_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeVXil1qdQGj",
        "outputId": "c520ffb0-d0fa-4163-89c9-25f02cb6e5be"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** GCN training/testing on dataset chameleon **\n",
            "** GCN training/testing on dataset cornell **\n",
            "** GCN training/testing on dataset texas **\n",
            "** GCN training/testing on dataset wisconsin **\n",
            "+-----------+---------+------------+-----------------+\n",
            "| Dataset   | Split   |   Time (s) | Test Accuracy   |\n",
            "+===========+=========+============+=================+\n",
            "| chameleon | 0       |       0.52 | 56.9 +/- 1.7    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 1       |       0.51 | 59.6 +/- 1.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 2       |       0.51 | 53.2 +/- 1.3    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 3       |       0.51 | 59.0 +/- 1.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 4       |       0.5  | 55.5 +/- 1.3    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 5       |       0.51 | 56.5 +/- 2.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 6       |       0.51 | 52.5 +/- 1.2    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 7       |       0.52 | 53.1 +/- 0.9    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 8       |       0.51 | 55.7 +/- 1.2    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | 9       |       0.51 | 56.6 +/- 1.3    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 0       |       0.42 | 59.2 +/- 0.8    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 1       |       0.48 | 60.8 +/- 1.6    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 2       |       0.48 | 62.6 +/- 2.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 3       |       0.43 | 50.9 +/- 1.5    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 4       |       0.47 | 62.7 +/- 1.8    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 5       |       0.48 | 64.3 +/- 1.4    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 6       |       0.43 | 57.0 +/- 1.9    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 7       |       0.47 | 61.2 +/- 2.7    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 8       |       0.48 | 64.2 +/- 3.4    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | 9       |       0.44 | 53.9 +/- 2.5    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 0       |       0.48 | 59.5 +/- 0.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 1       |       0.48 | 49.6 +/- 1.3    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 2       |       0.44 | 46.4 +/- 2.5    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 3       |       0.46 | 62.7 +/- 4.4    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 4       |       0.48 | 51.1 +/- 2.2    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 5       |       0.45 | 46.9 +/- 2.5    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 6       |       0.46 | 48.4 +/- 1.7    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 7       |       0.48 | 44.1 +/- 2.7    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 8       |       0.45 | 52.2 +/- 3.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | 9       |       0.46 | 66.5 +/- 3.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 0       |       0.47 | 43.8 +/- 2.7    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 1       |       0.46 | 51.4 +/- 2.9    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 2       |       0.45 | 49.9 +/- 3.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 3       |       0.48 | 45.2 +/- 2.4    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 4       |       0.48 | 46.4 +/- 2.3    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 5       |       0.43 | 47.6 +/- 2.4    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 6       |       0.48 | 49.5 +/- 1.4    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 7       |       0.48 | 50.0 +/- 2.4    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 8       |       0.45 | 53.9 +/- 2.1    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | 9       |       0.47 | 47.3 +/- 1.8    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| chameleon | Average |       0.51 | 55.9 +/- 1.3    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| cornell   | Average |       0.46 | 59.7 +/- 2.0    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| texas     | Average |       0.46 | 52.7 +/- 2.3    |\n",
            "+-----------+---------+------------+-----------------+\n",
            "| wisconsin | Average |       0.47 | 48.5 +/- 2.4    |\n",
            "+-----------+---------+------------+-----------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. Licenses for referenced code\n",
        "\n",
        "[SGC LICENSE](https://github.com/Tiiiger/SGC/blob/master/LICENSE):\n",
        "```\n",
        "The MIT License\n",
        "\n",
        "Copyright (c) 2019 Tianyi Zhang\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in\n",
        "all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "THE SOFTWARE.\n",
        "```\n",
        "\n",
        "[Geom-GCN LICENSE](https://github.com/bingzhewei/geom-gcn/blob/master/utils_data.py)\n",
        "```\n",
        "#  MIT License\n",
        "#\n",
        "#  Copyright (c) 2019 Geom-GCN Authors\n",
        "#\n",
        "#  Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "#  of this software and associated documentation files (the \"Software\"), to deal\n",
        "#  in the Software without restriction, including without limitation the rights\n",
        "#  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "#  copies of the Software, and to permit persons to whom the Software is\n",
        "#  furnished to do so, subject to the following conditions:\n",
        "#\n",
        "#  The above copyright notice and this permission notice shall be included in all\n",
        "#  copies or substantial portions of the Software.\n",
        "#\n",
        "#  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "#  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "#  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "#  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "#  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "#  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "#  SOFTWARE.\n",
        "```"
      ],
      "metadata": {
        "id": "UW7IevgHLu0a"
      }
    }
  ]
}
